# -*- coding: utf-8 -*-
"""Disaster_Tweets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l8x0uMCLs7bNwGfyRjxJm9gmU3AXZwBf
"""

# Commented out IPython magic to ensure Python compatibility.
# libraries do not need to be mentioned in report
import numpy as np
import pandas as pd
# %matplotlib inline
from matplotlib import pyplot as plt

!pip install -q -U "tensorflow==2.11.*"
!pip install -q -U "tensorflow-text==2.11.*"
!pip install -q -U "tf-models-official==2.11.*"

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
from official.nlp import optimization

# Run this unit only if you are using Colab
# To make sure this works, go to "shared with me" on your google drive, right click on the folder 'CS441 Project'
# click on add short cut to drive, and put the short cut in 'My Drive'

from google.colab import drive

drive.mount('/content/drive')
datadir = "/content/drive/My Drive/CS441_Project/Data/"

bert_model_name = 'bert_en_cased_L-12_H-768_A-12'  #@param ["bert_en_uncased_L-12_H-768_A-12", "bert_en_cased_L-12_H-768_A-12", "bert_multi_cased_L-12_H-768_A-12", "small_bert/bert_en_uncased_L-2_H-128_A-2", "small_bert/bert_en_uncased_L-2_H-256_A-4", "small_bert/bert_en_uncased_L-2_H-512_A-8", "small_bert/bert_en_uncased_L-2_H-768_A-12", "small_bert/bert_en_uncased_L-4_H-128_A-2", "small_bert/bert_en_uncased_L-4_H-256_A-4", "small_bert/bert_en_uncased_L-4_H-512_A-8", "small_bert/bert_en_uncased_L-4_H-768_A-12", "small_bert/bert_en_uncased_L-6_H-128_A-2", "small_bert/bert_en_uncased_L-6_H-256_A-4", "small_bert/bert_en_uncased_L-6_H-512_A-8", "small_bert/bert_en_uncased_L-6_H-768_A-12", "small_bert/bert_en_uncased_L-8_H-128_A-2", "small_bert/bert_en_uncased_L-8_H-256_A-4", "small_bert/bert_en_uncased_L-8_H-512_A-8", "small_bert/bert_en_uncased_L-8_H-768_A-12", "small_bert/bert_en_uncased_L-10_H-128_A-2", "small_bert/bert_en_uncased_L-10_H-256_A-4", "small_bert/bert_en_uncased_L-10_H-512_A-8", "small_bert/bert_en_uncased_L-10_H-768_A-12", "small_bert/bert_en_uncased_L-12_H-128_A-2", "small_bert/bert_en_uncased_L-12_H-256_A-4", "small_bert/bert_en_uncased_L-12_H-512_A-8", "small_bert/bert_en_uncased_L-12_H-768_A-12", "albert_en_base", "electra_small", "electra_base", "experts_pubmed", "experts_wiki_books", "talking-heads_base"]

map_name_to_handle = {
    'bert_en_uncased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',
    'bert_en_cased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',
    'bert_multi_cased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',
    'small_bert/bert_en_uncased_L-2_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-2_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-2_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-2_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-4_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-4_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-4_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-4_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-6_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-6_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-6_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-6_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-8_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-8_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-8_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-8_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-10_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-10_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-10_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-10_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-12_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-12_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-12_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',
    'albert_en_base':
        'https://tfhub.dev/tensorflow/albert_en_base/2',
    'electra_small':
        'https://tfhub.dev/google/electra_small/2',
    'electra_base':
        'https://tfhub.dev/google/electra_base/2',
    'experts_pubmed':
        'https://tfhub.dev/google/experts/bert/pubmed/2',
    'experts_wiki_books':
        'https://tfhub.dev/google/experts/bert/wiki_books/2',
    'talking-heads_base':
        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',
}

map_model_to_preprocess = {
    'bert_en_uncased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'bert_en_cased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',
    'small_bert/bert_en_uncased_L-2_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-2_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-2_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-2_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-4_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-4_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-4_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-4_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-6_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-6_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-6_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-6_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-8_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-8_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-8_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-8_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-10_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-10_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-10_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-10_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-12_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-12_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-12_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'bert_multi_cased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',
    'albert_en_base':
        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',
    'electra_small':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'electra_base':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'experts_pubmed':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'experts_wiki_books':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'talking-heads_base':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
}

tfhub_encoder = map_name_to_handle[bert_model_name]
tfhub_preprocess = map_model_to_preprocess[bert_model_name]

def load_data():
  train_df = pd.read_csv(datadir + 'train.csv')
  test_df = pd.read_csv(datadir + 'test.csv')
  train_df_key = train_df.dropna(subset=['keyword'])
  test_df_key = test_df.dropna(subset=['keyword'])
  train_df_location = train_df.dropna(subset=['location'])
  test_df_location = test_df.dropna(subset=['location'])

  train_text = train_df['text'].values
  test_text = test_df['text'].values
  train_y = train_df['target']

  train_key = train_df_key['keyword'].values
  test_key = test_df_key['keyword'].values
  train_y_key = train_df_key['target']

  train_location = train_df_location['location'].values
  test_location = test_df_location['location'].values
  train_y_location = train_df_location['target']

  train_ds = tf.data.Dataset.from_tensor_slices((train_text, train_y))
  test_ds = tf.data.Dataset.from_tensor_slices(test_text)

  train_key_ds = tf.data.Dataset.from_tensor_slices((train_key, train_y_key))
  test_key_ds = tf.data.Dataset.from_tensor_slices(test_key)

  train_location_ds = tf.data.Dataset.from_tensor_slices((train_location, train_y_location))
  test_location_ds = tf.data.Dataset.from_tensor_slices(test_location)                                         

  return train_ds, test_ds, train_key_ds, test_key_ds, train_location_ds, test_location_ds

(train_ds, test_ds, train_key_ds, test_key_ds, train_location_ds, test_location_ds) = load_data()

def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  preprocessing_layer = hub.KerasLayer(tfhub_preprocess, name='preprocessing')
  encoder_inputs = preprocessing_layer(text_input)
  encoder = hub.KerasLayer(tfhub_encoder, trainable=True, name='BERT_encoder')
  outputs = encoder(encoder_inputs)
  net = outputs['pooled_output']
  net = tf.keras.layers.Dropout(0.1)(net)
  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)
  return tf.keras.Model(text_input, net)

BATCH_SIZE = 64
SHUFFLE_BUFFER_SIZE = 100

train_ds = train_ds.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
test_ds = test_ds.batch(BATCH_SIZE)

train_key_ds = train_key_ds.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
test_key_ds = test_key_ds.batch(BATCH_SIZE)

train_location_ds = train_location_ds.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
test_location_ds = test_location_ds.batch(BATCH_SIZE)

loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)
metrics = tf.metrics.BinaryAccuracy()
epochs = 5
steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()
num_train_steps = steps_per_epoch * epochs
num_warmup_steps = int(0.1*num_train_steps)

init_lr = 3e-5
optimizer = optimization.create_optimizer(init_lr=init_lr, num_train_steps=num_train_steps,num_warmup_steps=num_warmup_steps,optimizer_type='adamw')

model = build_classifier_model()  
model.compile(optimizer=optimizer,loss=loss,metrics=metrics)

print(f'Training model with {tfhub_encoder}')
history = model.fit(x=train_ds,epochs=epochs)

history_dict = history.history

acc = history_dict['binary_accuracy']
loss = history_dict['loss']

epochs = range(1, len(acc) + 1)
fig = plt.figure(figsize=(10, 6))
fig.tight_layout()


plt.plot(epochs, loss, 'r', label='Training loss')
plt.title('Training loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.plot(epochs, acc, 'b', label='Training acc')
plt.title('Training ccuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()

modeldir = "/content/drive/My Drive/CS441_Project/Models/"
model.save(modeldir, include_optimizer=True)

result = (tf.sigmoid(model.predict(train_ds)) > 0.5).numpy()

result.shape
np.sum(result)

modeldir = "/content/drive/My Drive/CS441_Project/Models/"

model_keywords = build_classifier_model()
model_keywords.compile(optimizer=optimizer,loss=loss,metrics=metrics)

print(f'Training model with {tfhub_encoder}')
history_keywords = model_keywords.fit(x=train_key_ds,epochs=7)

history_dict = history_keywords.history

acc = history_dict['binary_accuracy']
loss = history_dict['loss']

epochs = range(1, len(acc) + 1)
fig = plt.figure(figsize=(10, 6))
fig.tight_layout()


plt.plot(epochs, loss, 'r', label='Training loss')
plt.title('Training loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.plot(epochs, acc, 'b', label='Training acc')
plt.title('Training ccuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()

modeldir_keywords = "/content/drive/My Drive/CS441_Project/Model_keyword/"

model_keywords.save(modeldir_keywords, include_optimizer=True)

model_location = build_classifier_model()
model_location.compile(optimizer=optimizer,loss=loss,metrics=metrics)

print(f'Training model with {tfhub_encoder}')
history_location = model_location.fit(x=train_location_ds,epochs=4)

history_dict = history_location.history

acc = history_dict['binary_accuracy']
loss = history_dict['loss']

epochs = range(1, len(acc) + 1)
fig = plt.figure(figsize=(10, 6))
fig.tight_layout()


plt.plot(epochs, loss, 'r', label='Training loss')
plt.title('Training loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.plot(epochs, acc, 'b', label='Training acc')
plt.title('Training ccuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()

modeldir_location = "/content/drive/My Drive/CS441_Project/Model_location/"

model_location.save(modeldir_location, include_optimizer=True)

cached_model = tf.saved_model.load(modeldir)

cached_model_keywords = tf.saved_model.load(modeldir_keywords)

cached_model_location = tf.saved_model.load(modeldir_location)

## General Model Classification Test

import pandas as pd
import numpy as np
import tensorflow as tf

datadir = "/content/drive/My Drive/CS441_Project/Data/"
modeldir = "/content/drive/My Drive/CS441_Project/Models/"
submissiondir = "/content/drive/My Drive/CS441_Project/"

# Load the saved model
model = tf.keras.models.load_model(modeldir, compile=False)

# Load the test dataset
test_data = pd.read_csv(datadir + 'test.csv')

# Preprocess the test data
test_text = test_data['text'].tolist()
test_text = [str(text) for text in test_text]

# Predict the class labels on the test data
test_ds = tf.data.Dataset.from_tensor_slices(test_text).batch(64)
test_pred = (tf.sigmoid(model.predict(test_ds)) > 0.5).numpy().flatten()

# Create a submission file with the predicted targets
submission = pd.DataFrame({'id': test_data['id'], 'target': test_pred.astype(np.int32)})
submission.to_csv(submissiondir + 'submission.csv', index=False)

## Keyword Model Classification Test

modeldir_keywords = "/content/drive/My Drive/CS441_Project/Model_keyword/"

# Load the saved model
model = tf.keras.models.load_model(modeldir_keywords, compile=False)

# Load the test dataset
test_data = pd.read_csv(datadir + 'test.csv')

# Preprocess the test data
test_text = test_data['text'].tolist()
test_text = [str(text) for text in test_text]

# Predict the class labels on the test data
test_ds = tf.data.Dataset.from_tensor_slices(test_text).batch(64)
test_pred = (tf.sigmoid(model.predict(test_ds)) > 0.5).numpy().flatten()

# Create a submission file with the predicted targets
submission = pd.DataFrame({'id': test_data['id'], 'target': test_pred.astype(np.int32)})
submission.to_csv(submissiondir + 'submission_keywords.csv', index=False)

## Location Model Classification Test

modeldir_location = "/content/drive/My Drive/CS441_Project/Model_location/"

# Load the saved model
model = tf.keras.models.load_model(modeldir_location, compile=False)

# Load the test dataset
test_data = pd.read_csv(datadir + 'test.csv')

# Preprocess the test data
test_text = test_data['text'].tolist()
test_text = [str(text) for text in test_text]

# Predict the class labels on the test data
test_ds = tf.data.Dataset.from_tensor_slices(test_text).batch(64)
test_pred = (tf.sigmoid(model.predict(test_ds)) > 0.5).numpy().flatten()

# Create a submission file with the predicted targets
submission = pd.DataFrame({'id': test_data['id'], 'target': test_pred.astype(np.int32)})
submission.to_csv(submissiondir + 'submission_location.csv', index=False)

## All Models Classification Test

# Load the saved model
model = tf.keras.models.load_model(modeldir, compile=False)
model_keywords = tf.keras.models.load_model(modeldir_keywords, compile=False)
model_location = tf.keras.models.load_model(modeldir_location, compile=False)

# Load the test dataset
test_data = pd.read_csv(datadir + 'test.csv')

# Preprocess the test data
test_text = test_data['text'].tolist()
test_text = [str(text) for text in test_text]

# Predict the class labels on the test data using each model
test_ds = tf.data.Dataset.from_tensor_slices(test_text).batch(64)
test_pred_general = (tf.sigmoid(model.predict(test_ds)) > 0.5).numpy().flatten()
test_pred_keyword = (tf.sigmoid(model_keywords.predict(test_ds)) > 0.5).numpy().flatten()
test_pred_location = (tf.sigmoid(model_location.predict(test_ds)) > 0.5).numpy().flatten()

# Combine the predictions of the three models using majority voting
test_pred = np.zeros_like(test_pred_general)
for i in range(len(test_pred)):
    votes = [test_pred_general[i], test_pred_keyword[i], test_pred_location[i]]
    if votes.count(0) > votes.count(1):
        test_pred[i] = 0
    else:
        test_pred[i] = 1

# Create a submission file with the predicted targets
submission = pd.DataFrame({'id': test_data['id'], 'target': test_pred.astype(np.int32)})
submission.to_csv(submissiondir + 'submission_combine.csv', index=False)